{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz = np.load('Audiobooks_data_train.npz')\n",
    "train_inputs, train_targets = npz['inputs'].astype(float), npz['targets'].astype(int)\n",
    "\n",
    "npz = np.load('Audiobooks_data_validation.npz')\n",
    "validation_inputs, validation_targets = npz['inputs'].astype(float), npz['targets'].astype(int)\n",
    "\n",
    "npz = np.load('Audiobooks_data_test.npz')\n",
    "test_inputs, test_targets = npz['inputs'].astype(float), npz['targets'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(name, history):\n",
    "  plt.title(name +' - '+ 'Train vs Validation Losses')\n",
    "  train_losses = history.history['loss']\n",
    "  val_losses = history.history['val_loss']\n",
    "  plt.plot(train_losses,'-o')\n",
    "  plt.plot(val_losses,'-o')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Losses')\n",
    "  plt.legend(['Train Losses','Validation Losses'])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs=100, batch_size=9, optimizer=\"adam\", sequential= [], patience = 2):\n",
    "  model = tf.keras.Sequential(sequential)\n",
    "  model.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
    "  history = model.fit(train_inputs,train_targets, \n",
    "            batch_size= batch_size, epochs = epochs,\n",
    "            callbacks= [tf.keras.callbacks.EarlyStopping(patience =patience)],\n",
    "            validation_data=(validation_inputs, validation_targets),\n",
    "            validation_steps=10,\n",
    "            verbose = 0\n",
    "          )\n",
    "  test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)\n",
    "  return test_loss, test_accuracy , history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 10\n",
    "output_size = 2\n",
    "hidden_nodes = 8\n",
    "sequential = []\n",
    "for i in range(3):\n",
    "  sequential.append(tf.keras.layers.Dense(hidden_nodes, activation='relu'))\n",
    "sequential.append(tf.keras.layers.Dense(output_size, activation='softmax'))\n",
    "test_loss, test_accuracy, history = train(epochs=100, batch_size=9, optimizer=\"adam\", \n",
    "                                          sequential= sequential, patience = 2)\n",
    "print('\\nTest loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))\n",
    "plot_loss(\"\", history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "14/14 [==============================] - 0s 2ms/step - loss: 0.1885 - accuracy: 0.9442\n",
    "\n",
    "Test loss: 0.19. Test accuracy: 94.42%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_size = 10\n",
    "output_size = 2\n",
    "hidden_nodes = 8\n",
    "sequential = []\n",
    "for i in range(2):\n",
    "  sequential.append(tf.keras.layers.Dense(hidden_nodes, activation='relu'))\n",
    "sequential.append(tf.keras.layers.Dense(output_size, activation='softmax'))\n",
    "\n",
    "# \"Adagrad\",\"Adadelta\",\"Adam\",\"Adamax\",\"SGD\",\"Nadam\",\"RMSprop\",\"FTRL\"\n",
    "optimizers = [\"Adagrad\",\"Adadelta\",\"Adam\",\"Adamax\",\"SGD\",\"RMSprop\"]\n",
    "histories = []\n",
    "test_accuracies = []\n",
    "for i, opt in enumerate(optimizers):\n",
    "  test_loss, test_accuracy, history = train(epochs=100, batch_size=9, optimizer=optimizers[i], \n",
    "                                          sequential= sequential, patience = 1)\n",
    "  print(\"Optimizers:\"+ optimizers[i])\n",
    "  print('\\nTest loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))\n",
    "  histories.append(history)\n",
    "  test_accuracies.append(test_accuracy)\n",
    "  plot_loss(optimizers[i],history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Adagrad\",\"Adadelta\",\"Adam\",\"Adamax\",\"SGD\",\"Nadam\",\"RMSprop\",\"FTRL\"\n",
    "# optimizers = [\"Adagrad\",\"Adadelta\",\"Adam\",\"Adamax\",\"SGD\",\"RMSprop\"]\n",
    "# histories = []\n",
    "# test_accuracies = []\n",
    "# for i, opt in enumerate(optimizers):\n",
    "#   test_loss, test_accuracy, history = train(epochs=100, batch_size=9, optimizer=opt, sequential= sequential, patience = 5)\n",
    "#   print(\"Optimizers:\"+ optimizers[i])\n",
    "#   print('\\nTest loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))\n",
    "#   histories.append(history)\n",
    "#   test_accuracies.append(test_accuracy)\n",
    "#   plot_loss(optimizers[i],history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(2, 3)\n",
    "# val_acc = history.history['val_accuracy']\n",
    "# val_loss = history.history['val_loss']\n",
    "\n",
    "# fig.set_size_inches(10, 5)\n",
    "# ax[0][0].plot(train_loss, '-o', label='Training Loss')\n",
    "# ax[0][0].plot(val_loss, '-o', label='Validation Loss')\n",
    "# ax[0][0].set_title('Training & Validation Loss')\n",
    "# ax[0][0].legend()\n",
    "# ax[0][0].set_xlabel(\"Epochs\")\n",
    "# ax[0][0].set_ylabel(\"Losses\")\n",
    "\n",
    "# ax[1][0].plot(train_loss, '-o', label='Training Loss')\n",
    "# ax[1][0].plot(val_loss, '-o', label='Validation Loss')\n",
    "# ax[1][0].set_title('Training & Validation Loss')\n",
    "# ax[1][0].legend()\n",
    "# ax[1][0].set_xlabel(\"Epochs\")\n",
    "# ax[1][0].set_ylabel(\"Losses\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, optimizer in enumerate(optimizers):\n",
    "  print('----------------------------')\n",
    "  print(\"Optimizer: \"+ optimizer)\n",
    "  print('Test accuracy: {0:.2f}%'.format(test_accuracies[i]*100.))\n",
    "  plot_loss(optimizer,histories[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(history):\n",
    "  fig , ax = plt.subplots(1,2)\n",
    "  train_acc = history.history['accuracy']\n",
    "  train_loss = history.history['loss']\n",
    "  val_acc = history.history['val_accuracy']\n",
    "  val_loss = history.history['val_loss']\n",
    "  fig.set_size_inches(10,5)\n",
    "  ax[0].plot(train_acc , '-o' , label = 'Training Accuracy')\n",
    "  ax[0].plot(val_acc , '-o' , label = 'Validation Accuracy')\n",
    "  ax[0].set_title('Training & Validation Accuracy')\n",
    "  ax[0].legend()\n",
    "  ax[0].set_xlabel(\"Epochs\")\n",
    "  ax[0].set_ylabel(\"Accuracy\")\n",
    "\n",
    "  ax[1].plot(train_loss , '-o' , label = 'Training Loss')\n",
    "  ax[1].plot(val_loss , '-o' , label = 'Validation Loss')\n",
    "  ax[1].set_title('Training & Validation Loss')\n",
    "  ax[1].legend()\n",
    "  ax[1].set_xlabel(\"Epochs\")\n",
    "  ax[1].set_ylabel(\"Losses\")\n",
    "  plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
